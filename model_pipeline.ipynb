{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107e4a0e-f06e-4860-b7d2-5d4a7e6b362f",
   "metadata": {},
   "source": [
    "# Model Pipeline\n",
    "\n",
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82152d6f-aa33-4412-82f9-ce24e50d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "#import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e8befd-c4b9-4ec9-8c48-27c32ded23bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667c2595-5b47-4701-90cb-307d9ab1bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-728406898807/airdata/sensor_data.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/sensor_data.csv\"\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/airdata\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec58ebf2-0092-4a54-a5e5-6a2dd287e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6219f-e675-4925-b3b4-54cf1650fbf6",
   "metadata": {},
   "source": [
    "## Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce99344-a8fd-4aba-b6f7-103c475a8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TARGET_PARAM = \"pm25\"\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "df_location = pd.read_csv(\n",
    "    f\"{base_dir}/input/sensor_data.csv\"\n",
    ")\n",
    "\n",
    "# Split training\n",
    "df_param = df_location[df_location['parameter'] == TARGET_PARAM]  # Filter data for this parameter\n",
    "train_data = df_param.iloc[:int(len(df_param) * TRAIN_SPLIT)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split validation\n",
    "val_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT)]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "# Split testing\n",
    "test_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT) + int(len(df_param) * TEST_SPLIT)]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "val_data.loc[:, \"value\"] = scaler.transform(val_data[\"value\"].values.reshape(-1, 1))\n",
    "test_data.loc[:, \"value\"] = scaler.transform(test_data[\"value\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "pd.DataFrame(train_data).to_csv(f\"{base_dir}/train/train.csv\")\n",
    "pd.DataFrame(val_data).to_csv(f\"{base_dir}/validation/validation.csv\")\n",
    "pd.DataFrame(test_data).to_csv(f\"{base_dir}/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8d4734-6ac1-4c92-9341-c7f3751f11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-airdata-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff14b4d6-2f66-49c2-bacf-67c2f49fff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"AirDataProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e8073-366c-4d02-ace6-d6025a7eda0b",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "\n",
    "After processing features are uploaded to a Feature Store so they can be accessed in later projects or when re-training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74837e7a-6936-4f03-a47e-387fca8935d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\",\n",
    "                                       region_name=region)\n",
    "\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa29b9e1-e4cf-4b99-af0e-58da3b7e52e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-728406898807\n"
     ]
    }
   ],
   "source": [
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = \"airdata-featurestore\"\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78976848-f68b-471a-8ea7-8a0fd1184f76",
   "metadata": {},
   "source": [
    "#### Process Features\n",
    "\n",
    "Pre-process and perform feature engineering before uploading features to Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a922d61e-ad88-410c-a0e3-efda34645029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (700, 11)\n",
      "Validation Data Shape: (150, 11)\n",
      "Test Data Shape: (150, 11)\n"
     ]
    }
   ],
   "source": [
    "# Run pre-processor once\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TARGET_PARAM = \"pm25\"\n",
    "\n",
    "base_dir = \"data\"\n",
    "\n",
    "df_location = pd.read_csv(\n",
    "    f\"{base_dir}/sensor_data.csv\"\n",
    ")\n",
    "\n",
    "# Split training\n",
    "df_param = df_location[df_location['parameter'] == TARGET_PARAM]  # Filter data for this parameter\n",
    "train_data = df_param.iloc[:int(len(df_param) * TRAIN_SPLIT)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split validation\n",
    "val_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT)]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "# Split testing\n",
    "test_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT) + int(len(df_param) * TEST_SPLIT)]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "val_data.loc[:, \"value\"] = scaler.transform(val_data[\"value\"].values.reshape(-1, 1))\n",
    "test_data.loc[:, \"value\"] = scaler.transform(test_data[\"value\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d895ce9-c16b-450f-9b4d-845b6de8c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change id to string for better lookup\n",
    "\n",
    "train_data[\"measurement_id\"] = train_data[\"measurement_id\"].astype(str)  # Convert before ingestion\n",
    "val_data[\"measurement_id\"] = val_data[\"measurement_id\"].astype(str)  # Convert before ingestion\n",
    "test_data[\"measurement_id\"] = test_data[\"measurement_id\"].astype(str)  # Convert before ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e83bf-fbcb-4c60-92bc-7b86e62433ad",
   "metadata": {},
   "source": [
    "### Define Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e06ff65-9df9-4767-9169-5be0440b8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "train_feature_group_name = \"train-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "val_feature_group_name = \"val-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "test_feature_group_name = \"test-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52a5d63-de79-45ad-a2d7-e88f46f50355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "train_feature_group = FeatureGroup(\n",
    "    name=train_feature_group_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "val_feature_group = FeatureGroup(\n",
    "    name=val_feature_group_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "test_feature_group = FeatureGroup(\n",
    "    name=test_feature_group_name, sagemaker_session=feature_store_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1872aa3-01da-47e3-9b9e-a206d597a83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='measurement_id', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='sensor_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='location_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='location', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='latitude', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='longitude', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='epoch', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='duration', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='parameter', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='value', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='units', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='EventTime', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "\n",
    "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
    "cast_object_to_string(train_data)\n",
    "cast_object_to_string(val_data)\n",
    "cast_object_to_string(test_data)\n",
    "\n",
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"measurement_id\"\n",
    "event_time_feature_name = \"EventTime\"\n",
    "\n",
    "# append EventTime feature\n",
    "train_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(train_data), dtype=\"float64\"\n",
    ")\n",
    "val_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(val_data), dtype=\"float64\"\n",
    ")\n",
    "test_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(val_data), dtype=\"float64\"\n",
    ")\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "train_feature_group.load_feature_definitions(data_frame=train_data)\n",
    "# output is suppressed\n",
    "val_feature_group.load_feature_definitions(data_frame=val_data)\n",
    "# output is suppressed\n",
    "test_feature_group.load_feature_definitions(data_frame=test_data)\n",
    "# output is suppressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa126a01-8025-4789-b61f-daa33576413c",
   "metadata": {},
   "source": [
    "### Create FeatureGroups in SageMaker FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860d5855-7189-403e-8cb1-096126589f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup train-feature-group-19-22-47-28 successfully created.\n",
      "FeatureGroup val-feature-group-19-22-47-28 successfully created.\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup test-feature-group-19-22-47-28 successfully created.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "train_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "val_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "test_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=train_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=val_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=test_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28cdd00-9016-4251-8567-10e0c25afdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-22-47-28',\n",
       " 'FeatureGroupName': 'train-feature-group-19-22-47-28',\n",
       " 'RecordIdentifierFeatureName': 'measurement_id',\n",
       " 'EventTimeFeatureName': 'EventTime',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'measurement_id',\n",
       "   'FeatureType': 'String'},\n",
       "  {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'location_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'location', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'latitude', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'longitude', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'epoch', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'duration', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'parameter', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'value', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'units', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2025, 2, 19, 22, 47, 28, 590000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-728406898807/airdata-featurestore',\n",
       "   'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-728406898807/airdata-featurestore/728406898807/sagemaker/us-east-1/offline-store/train-feature-group-19-22-47-28-1740005248/data'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'train_feature_group_19_22_47_28_1740005248',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'ThroughputConfig': {'ThroughputMode': 'OnDemand'},\n",
       " 'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'OnlineStoreTotalSizeBytes': 0,\n",
       " 'ResponseMetadata': {'RequestId': '6149f094-59d6-4163-b1f7-7a9013d03622',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6149f094-59d6-4163-b1f7-7a9013d03622',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2057',\n",
       "   'date': 'Wed, 19 Feb 2025 22:48:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcc6f26-491f-49d8-b02a-d6eac14255fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'val-feature-group-19-22-47-28',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/val-feature-group-19-22-47-28',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 22, 47, 30, 267000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created'},\n",
       "  {'FeatureGroupName': 'val-feature-group-19-22-38-29',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/val-feature-group-19-22-38-29',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 22, 38, 39, 697000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'val-feature-group-19-00-42-14',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/val-feature-group-19-00-42-14',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 0, 42, 15, 804000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'val-feature-group-19-00-34-04',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/val-feature-group-19-00-34-04',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 0, 34, 6, 432000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created'},\n",
       "  {'FeatureGroupName': 'val-feature-group-18-23-34-29',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/val-feature-group-18-23-34-29',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 18, 23, 54, 52, 558000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'train-feature-group-19-22-47-28',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-22-47-28',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 22, 47, 28, 590000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created'},\n",
       "  {'FeatureGroupName': 'train-feature-group-19-22-38-29',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-22-38-29',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 22, 38, 38, 271000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'train-feature-group-19-00-42-14',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-00-42-14',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 0, 42, 14, 939000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'train-feature-group-19-00-34-04',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-00-34-04',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 19, 0, 34, 4, 816000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created'},\n",
       "  {'FeatureGroupName': 'train-feature-group-18-23-34-29',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-18-23-34-29',\n",
       "   'CreationTime': datetime.datetime(2025, 2, 18, 23, 54, 51, 401000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}}],\n",
       " 'NextToken': 'cIws2QhTXUIa8bi8WqgA+NmGiHRIgZqI76bVgWXBfu2I673LVzr4bncGJ231ZuezuO/Z8l9Xpg0nl6sbltkMiqYJvwgA9EOSnxJeXTqGDFJCqa+xiW0obRullRHsQpUZ44YCXkWv/KU/jYsBPEyDn3u9OCmLXqNDViFHfuKEHwQiXb198zZUTk5O3gLa7JKHPu5qVDyR6FDgWzrJBdWoUzs4i3UcgUesnVQp5gTBA7eJmvN3NcJdp9yqWoTzDiIEAGwzEMK4OUdLpU7Z0z/S+KjT9J7ab4zAjyly5VUXDWfi9aUKXVu/XyEvPFRQxQ2YBPVVk+dzfP5bo4tD7UtluELbf/+kOWWuq/qV7015RLv7I52mJU1KTTEmr9tBiYKy68GDevcE3bCI49EC4D+HfFY5fYlLEftBNRhcFOmczCsXkOcGKgbIwJraxynuJvleHz/Lz7ncbO4m8YBjUEHhDaKc/d/+zCrut5ZVRNsp1UP65cY/KDcR2OAVLxg6tjqnAG15RMq+0W5+GehymENgHi+rZKcjG38zO6cG/NW2kuKGIxA/C5C3HAHFZvq3EQ==',\n",
       " 'ResponseMetadata': {'RequestId': 'd23824b4-97e5-4cbf-9d8a-ea2d52da48ff',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd23824b4-97e5-4cbf-9d8a-ea2d52da48ff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3093',\n",
       "   'date': 'Wed, 19 Feb 2025 22:48:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client.list_feature_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de335d05-7e63-446b-be2d-36ac7481df84",
   "metadata": {},
   "source": [
    "### Put Records Into Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11577bb7-ad6c-4c8d-a09c-8a6ed0b48ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7875dda-28b6-4b0f-a703-f1c64f523415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='train-feature-group-19-22-47-28', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f5334622850>, sagemaker_session=<sagemaker.session.Session object at 0x7f53383f1590>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f5336074310>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_group.ingest(data_frame=train_data, max_workers=3, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "875b42a8-b7f2-407b-916f-44023001ef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='val-feature-group-19-22-47-28', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f5334622850>, sagemaker_session=<sagemaker.session.Session object at 0x7f53383f1590>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f5336039590>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_feature_group.ingest(data_frame=val_data, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e36be1bc-a62a-4af2-aade-e477873c23b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='test-feature-group-19-22-47-28', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f5334622850>, sagemaker_session=<sagemaker.session.Session object at 0x7f53383f1590>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f5336038890>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature_group.ingest(data_frame=test_data, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1d57d-411c-4c6c-aac1-866920b0f570",
   "metadata": {},
   "source": [
    "### Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e1caf9-703f-444f-946f-25b1ac91884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2025, 2, 19, 22, 47, 28, 590000, tzinfo=tzlocal()),\n",
      " 'EventTimeFeatureName': 'EventTime',\n",
      " 'FeatureDefinitions': [{'FeatureName': 'measurement_id',\n",
      "                         'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'},\n",
      "                        {'FeatureName': 'location_id',\n",
      "                         'FeatureType': 'Integral'},\n",
      "                        {'FeatureName': 'location', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'latitude',\n",
      "                         'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'longitude',\n",
      "                         'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'epoch', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'duration', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'parameter', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'value', 'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'units', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'EventTime',\n",
      "                         'FeatureType': 'Fractional'}],\n",
      " 'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:728406898807:feature-group/train-feature-group-19-22-47-28',\n",
      " 'FeatureGroupName': 'train-feature-group-19-22-47-28',\n",
      " 'FeatureGroupStatus': 'Created',\n",
      " 'OfflineStoreConfig': {'DataCatalogConfig': {'Catalog': 'AwsDataCatalog',\n",
      "                                              'Database': 'sagemaker_featurestore',\n",
      "                                              'TableName': 'train_feature_group_19_22_47_28_1740005248'},\n",
      "                        'DisableGlueTableCreation': False,\n",
      "                        'S3StorageConfig': {'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-728406898807/airdata-featurestore/728406898807/sagemaker/us-east-1/offline-store/train-feature-group-19-22-47-28-1740005248/data',\n",
      "                                            'S3Uri': 's3://sagemaker-us-east-1-728406898807/airdata-featurestore'}},\n",
      " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
      " 'OnlineStoreTotalSizeBytes': 0,\n",
      " 'RecordIdentifierFeatureName': 'measurement_id',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '2057',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Wed, 19 Feb 2025 22:48:24 GMT',\n",
      "                                      'x-amzn-requestid': 'ed4379b1-0a94-4a94-8343-e54e71250d74'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'ed4379b1-0a94-4a94-8343-e54e71250d74',\n",
      "                      'RetryAttempts': 0},\n",
      " 'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
      " 'ThroughputConfig': {'ThroughputMode': 'OnDemand'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "response = sagemaker_client.describe_feature_group(FeatureGroupName=train_feature_group_name)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd88e80-a1ff-41b6-ab4d-f6a5ca40629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Record': [{'FeatureName': 'measurement_id', 'ValueAsString': '2302'},\n",
      "            {'FeatureName': 'sensor_id', 'ValueAsString': '2000855'},\n",
      "            {'FeatureName': 'location_id', 'ValueAsString': '947312'},\n",
      "            {'FeatureName': 'location', 'ValueAsString': 'Canyon ES (2795)'},\n",
      "            {'FeatureName': 'latitude', 'ValueAsString': '34.03213'},\n",
      "            {'FeatureName': 'longitude', 'ValueAsString': '-118.51198'},\n",
      "            {'FeatureName': 'epoch', 'ValueAsString': '2022-02-19 06:36:52'},\n",
      "            {'FeatureName': 'duration', 'ValueAsString': '0 days 00:03:00'},\n",
      "            {'FeatureName': 'parameter', 'ValueAsString': 'pm25'},\n",
      "            {'FeatureName': 'value', 'ValueAsString': '-0.46964278273851745'},\n",
      "            {'FeatureName': 'units', 'ValueAsString': 'µg/m³'},\n",
      "            {'FeatureName': 'EventTime', 'ValueAsString': '1740005248.0'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1013',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Wed, 19 Feb 2025 22:55:01 GMT',\n",
      "                                      'x-amzn-requestid': '69ea1b02-708e-409e-89cf-56fc0dd9acad'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '69ea1b02-708e-409e-89cf-56fc0dd9acad',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "record_identifier_value = str(2302)\n",
    "\n",
    "record = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=train_feature_group_name,\n",
    "    RecordIdentifierValueAsString=record_identifier_value,\n",
    ")\n",
    "\n",
    "pprint.pprint(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e88c6-b394-4c0b-bc04-1e7bf535c4ba",
   "metadata": {},
   "source": [
    "A record can be successfully pulled from the Feature Store, indicating that features are available for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702dfd8-681d-40cf-b145-65250ed20dee",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92622913-be86-42ff-aa98-239d3c3f45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset definition\n",
    "# ---------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=30, pred_len=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data (pd.DataFrame or np.array): Time series with a column 'value'\n",
    "        - seq_len (int): Number of timesteps in the input sequence\n",
    "        - pred_len (int): Number of timesteps in the output sequence\n",
    "        \"\"\"\n",
    "        self.data = np.array(data[\"value\"]) if isinstance(data, pd.DataFrame) else np.array(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total number of sequences available\"\"\"\n",
    "        return max(0, len(self.data) - self.seq_len - self.pred_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves input sequence and target sequence\"\"\"\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset length {len(self)}\")\n",
    "\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len : idx + self.seq_len + self.pred_len]\n",
    "\n",
    "        # Return tensors for PyTorch\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# Model definition\n",
    "# ---------------------------\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        # out is (batch_size, seq_len, hidden_size)\n",
    "        # We want the last timestep\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # shape (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "# ---------------------------\n",
    "# Main training function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Channels for data paths (SageMaker will populate these automatically)\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--seq-len\", type=int, default=20)\n",
    "    parser.add_argument(\"--pred-len\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--hidden-size\", type=int, default=50)\n",
    "    parser.add_argument(\"--num-layers\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Load data\n",
    "    # ---------------------------\n",
    "    # Processor wrote \"train.csv\", \"validation.csv\", and \"test.csv\"\n",
    "    # into the respective directories: /opt/ml/input/data/train, etc.\n",
    "    train_csv = os.path.join(args.train, \"train.csv\")\n",
    "    val_csv   = os.path.join(args.validation, \"validation.csv\")\n",
    "    test_csv  = os.path.join(args.test, \"test.csv\")\n",
    "\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    val_data   = pd.read_csv(val_csv)\n",
    "    test_data  = pd.read_csv(test_csv)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Create PyTorch datasets & loaders\n",
    "    # ---------------------------\n",
    "    train_dataset = TimeSeriesDataset(train_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    val_dataset   = TimeSeriesDataset(val_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    test_dataset  = TimeSeriesDataset(test_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Initialize model, loss, optimizer\n",
    "    # ---------------------------\n",
    "    model = LSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training Loop\n",
    "    # ---------------------------\n",
    "    for epoch in range(args.epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_outputs = model(test_inputs)\n",
    "                test_loss += criterion(test_outputs, test_targets).item()\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{args.epochs}] \"\n",
    "              f\"TrainLoss: {train_loss:.4f} \"\n",
    "              f\"ValLoss: {val_loss:.4f} \"\n",
    "              f\"TestLoss: {test_loss:.4f}\")\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Save the model\n",
    "    # ---------------------------\n",
    "    # Uploads /opt/ml/model to S3 after training\n",
    "    model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "877b6006-ffe5-4d60-9c0f-4ff5f4cb1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters={\n",
    "        \"seq-len\": 20,\n",
    "        \"pred-len\": 1,\n",
    "        \"batch-size\": 8,\n",
    "        \"epochs\": 50,\n",
    "        \"hidden-size\": 50,\n",
    "        \"num-layers\": 1,\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    "    metric_definitions=[\n",
    "        {\n",
    "            \"Name\": \"test_mse\",\n",
    "            \"Regex\": r\"TestLoss:\\s+([0-9\\.]+)\"\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9757f1cb-2436-4b1a-b03c-dc41debe563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TrainStep\",\n",
    "    estimator=pytorch_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714b92b-3417-4ef8-a250-496c63483ab2",
   "metadata": {},
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ba76a1-617a-4696-b040-03314328a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        # out is (batch_size, seq_len, hidden_size)\n",
    "        # We want the last timestep\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # shape (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Loads the model from the model_dir. This is invoked by SageMaker once at\n",
    "    container startup to initialize your model. The returned object is passed\n",
    "    to `predict_fn` for every inference request.\n",
    "    \"\"\"\n",
    "    # Create model with the same architecture/hyperparams as training\n",
    "    model = LSTM(input_size=1, hidden_size=50, num_layers=1)\n",
    "    # Load state dict from model.pt\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserializes the incoming request body into a PyTorch tensor.\n",
    "    - If you expect JSON, parse it.\n",
    "    - If you expect CSV, parse differently, etc.\n",
    "    \"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        # Example: request_body = '{\"data\": [12.3, 45.6, 78.9, ...]}'\n",
    "        data = json.loads(request_body)[\"data\"]\n",
    "        # Convert to a float32 tensor. Suppose it's a 1D series (seq_len).\n",
    "        inputs = torch.tensor([data], dtype=torch.float32)\n",
    "        return inputs\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    \"\"\"\n",
    "    Performs prediction on the deserialized input.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # input_object shape = (batch=1, seq_len)\n",
    "        preds = model(input_object)  # shape = (batch=1, 1)\n",
    "    return preds\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Serializes the prediction output.\n",
    "    \"\"\"\n",
    "    if response_content_type == \"application/json\":\n",
    "        # Convert the tensor to a Python float\n",
    "        result = prediction.squeeze().item()  # single float\n",
    "        return json.dumps({\"prediction\": result})\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported response content type: {response_content_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2afe3020-97f4-4e37-97e5-71d6271260ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "inference_model = PyTorchModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    ")\n",
    "\n",
    "step_create = ModelStep(\n",
    "    name=\"AirDataCreateModel\",\n",
    "    step_args=inference_model.create(instance_type=\"ml.m5.large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b3f5ae7-c26c-4be8-bf51-59c09a45445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = None  # metrics we want?\n",
    "register_args = inference_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=\"AirDataModelGroup\",\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "step_register = ModelStep(\n",
    "    name=\"RegisterAirDataModel\",\n",
    "    step_args=register_args\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"AirDataRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd3892-0348-4c3b-99bf-d3fa07f7b28e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85486c31-2539-4c49-bf40-cf136968528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"MSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold])\n",
    ")\n",
    "\n",
    "test_mse = step_train.properties.FinalMetricDataList[0].Value\n",
    "\n",
    "step_check_loss = ConditionStep(\n",
    "    name=\"CheckTestLoss\",\n",
    "    conditions=[\n",
    "        ConditionLessThanOrEqualTo(\n",
    "            left=test_mse,\n",
    "            right=mse_threshold\n",
    "        )\n",
    "    ],\n",
    "    if_steps=[\n",
    "        step_create, step_register\n",
    "    ],\n",
    "    else_steps=[step_fail],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b22481-17c6-462d-979f-fc4cfa8a5f00",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "721c3b10-97dc-498f-a673-434e7e09e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"AirDataPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "        step_check_loss,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664e58bb-0ef1-46b9-9315-86ddb171a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-728406898807/airdata/sensor_data.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 0.5}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'AirDataProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-728406898807/AirDataPipeline/code/f6bb8f50726d68a202f70eb7594fdad0/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-728406898807',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-728406898807',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-728406898807',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'TrainStep',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0-cpu-py310',\n",
       "     'MetricDefinitions': [{'Name': 'test_mse',\n",
       "       'Regex': 'TestLoss:\\\\s+([0-9\\\\.]+)'}],\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'validation'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'seq-len': '20',\n",
       "     'pred-len': '1',\n",
       "     'batch-size': '8',\n",
       "     'epochs': '50',\n",
       "     'hidden-size': '50',\n",
       "     'num-layers': '1',\n",
       "     'lr': '0.001',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-728406898807/TrainStep-056a39253600e43f1578397e364413ee/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-east-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/',\n",
       "     'DisableProfiler': False}}},\n",
       "  {'Name': 'CheckTestLoss',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Get': 'Steps.TrainStep.FinalMetricDataList[0].Value'},\n",
       "      'RightValue': {'Get': 'Parameters.MseThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'AirDataCreateModel-RepackModel-0',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/pytorch-inference-2025-02-19-00-43-16-395'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "        'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large'},\n",
       "       'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': {'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'},\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"inference.py\"',\n",
       "        'model_archive': {'Std:Join': {'On': '',\n",
       "          'Values': [{'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': '\"code\"',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-728406898807/AirDataCreateModel-RepackModel-0-056a39253600e43f1578397e364413ee/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_region': '\"us-east-1\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/pytorch-inference-2025-02-19-00-43-16-395',\n",
       "        'CollectionConfigurations': []},\n",
       "       'ProfilerConfig': {'DisableProfiler': True}},\n",
       "      'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "     {'Name': 'AirDataCreateModel-CreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       "       'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0-cpu-py310',\n",
       "        'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "         'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "         'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "         'SAGEMAKER_REGION': 'us-east-1'},\n",
       "        'ModelDataUrl': {'Get': 'Steps.AirDataCreateModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "     {'Name': 'AirDataRegisterModel-RepackModel-0',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/pytorch-inference-2025-02-19-00-43-17-216'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "        'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large'},\n",
       "       'RoleArn': 'arn:aws:iam::728406898807:role/LabRole',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': {'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'},\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"inference.py\"',\n",
       "        'model_archive': {'Std:Join': {'On': '',\n",
       "          'Values': [{'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': '\"code\"',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-728406898807/AirDataRegisterModel-RepackModel-0-056a39253600e43f1578397e364413ee/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_region': '\"us-east-1\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-728406898807/pytorch-inference-2025-02-19-00-43-17-216',\n",
       "        'CollectionConfigurations': []},\n",
       "       'ProfilerConfig': {'DisableProfiler': True}},\n",
       "      'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "     {'Name': 'AirDataRegisterModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'AirDataModelGroup',\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0-cpu-py310',\n",
       "          'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "           'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "           'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "           'SAGEMAKER_REGION': 'us-east-1'},\n",
       "          'ModelDataUrl': {'Get': 'Steps.AirDataRegisterModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts'},\n",
       "          'Framework': 'PYTORCH',\n",
       "          'FrameworkVersion': '2.0'}],\n",
       "        'SupportedContentTypes': ['application/json'],\n",
       "        'SupportedResponseMIMETypes': ['application/json'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
       "       'SkipModelValidation': 'None'}}],\n",
       "    'ElseSteps': [{'Name': 'MSEFail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to MSE >',\n",
       "          {'Get': 'Parameters.MseThreshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb62c-20ad-41bf-9d18-46e48cfcd629",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3f8f37f-bf9b-4943-a5b1-62fe3ee2ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "try:\n",
    "    execution.wait()\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55336dbe-eb0a-4ee8-bcb3-476ae4e37cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:728406898807:pipeline/AirDataPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:728406898807:pipeline/AirDataPipeline/execution/37dgozc3o6h0',\n",
       " 'PipelineExecutionDisplayName': 'execution-1739925798619',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'airdatapipeline',\n",
       "  'TrialName': '37dgozc3o6h0'},\n",
       " 'CreationTime': datetime.datetime(2025, 2, 19, 0, 43, 18, 555000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 2, 19, 0, 51, 2, 787000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:728406898807:user-profile/d-0og3bvbn5ajw/kdevoe-studio-1',\n",
       "  'UserProfileName': 'kdevoe-studio-1',\n",
       "  'DomainId': 'd-0og3bvbn5ajw',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::728406898807:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA2TGDPHB33P5GE3SPR:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:728406898807:user-profile/d-0og3bvbn5ajw/kdevoe-studio-1',\n",
       "  'UserProfileName': 'kdevoe-studio-1',\n",
       "  'DomainId': 'd-0og3bvbn5ajw',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::728406898807:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA2TGDPHB33P5GE3SPR:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': '682b03a2-0750-4d1a-a336-f1fcea240b03',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '682b03a2-0750-4d1a-a336-f1fcea240b03',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1078',\n",
       "   'date': 'Wed, 19 Feb 2025 00:51:20 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36eca0e3-1d62-4569-9f13-1d40e8b291d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'AirDataCreateModel-CreateModel',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 51, 0, 250000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 51, 2, 435000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:model/pipelines-37dgozc3o6h0-AirDataCreateModel-C-quRFFKqOy1'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataRegisterModel-RegisterModel',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 50, 57, 705000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 50, 59, 963000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:model-package/AirDataModelGroup/1'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataCreateModel-RepackModel-0',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 48, 42, 992000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 50, 59, 552000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:training-job/pipelines-37dgozc3o6h0-AirDataCreateModel-R-WjDni1cbDk'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataRegisterModel-RepackModel-0',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 48, 42, 992000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 50, 57, 30000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:training-job/pipelines-37dgozc3o6h0-AirDataRegisterModel-mSuCOM7BjM'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'CheckTestLoss',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 48, 42, 3000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 48, 42, 277000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Condition': {'Outcome': 'True'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'TrainStep',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 45, 54, 78000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 48, 41, 335000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:training-job/pipelines-37dgozc3o6h0-TrainStep-Al0VPxjUpo'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataProcess',\n",
       "  'StartTime': datetime.datetime(2025, 2, 19, 0, 43, 19, 663000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 19, 0, 45, 53, 478000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:728406898807:processing-job/pipelines-37dgozc3o6h0-AirDataProcess-Opkz3BgJVv'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb574c0b-02e6-449a-a80b-544514074110",
   "metadata": {},
   "source": [
    "## Call Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7e27c-1f64-4954-93f2-f9ce2114b83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107e4a0e-f06e-4860-b7d2-5d4a7e6b362f",
   "metadata": {},
   "source": [
    "# Model Pipeline\n",
    "\n",
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82152d6f-aa33-4412-82f9-ce24e50d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e8befd-c4b9-4ec9-8c48-27c32ded23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74837e7a-6936-4f03-a47e-387fca8935d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-790237383528/airdata/sensor_data.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/sensor_data.csv\"\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/airdata\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec58ebf2-0092-4a54-a5e5-6a2dd287e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6219f-e675-4925-b3b4-54cf1650fbf6",
   "metadata": {},
   "source": [
    "## Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ce99344-a8fd-4aba-b6f7-103c475a8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TARGET_PARAM = \"pm25\"\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "df_location = pd.read_csv(\n",
    "    f\"{base_dir}/input/sensor_data.csv\"\n",
    ")\n",
    "\n",
    "# Split training\n",
    "df_param = df_location[df_location['parameter'] == TARGET_PARAM]  # Filter data for this parameter\n",
    "train_data = df_param.iloc[:int(len(df_param) * TRAIN_SPLIT)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split validation\n",
    "val_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT)]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "# Split testing\n",
    "test_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT) + int(len(df_param) * TEST_SPLIT)]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "val_data.loc[:, \"value\"] = scaler.transform(val_data[\"value\"].values.reshape(-1, 1))\n",
    "test_data.loc[:, \"value\"] = scaler.transform(test_data[\"value\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "pd.DataFrame(train_data).to_csv(f\"{base_dir}/train/train.csv\")\n",
    "pd.DataFrame(val_data).to_csv(f\"{base_dir}/validation/validation.csv\")\n",
    "pd.DataFrame(test_data).to_csv(f\"{base_dir}/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef8d4734-6ac1-4c92-9341-c7f3751f11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-airdata-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff14b4d6-2f66-49c2-bacf-67c2f49fff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"AirDataProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3427cf39-f2ef-4afb-bb1d-db9dfa274833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-790237383528/airdata/sensor_data.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 0.5}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'AirDataProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-790237383528/AirDataPipeline/code/f6bb8f50726d68a202f70eb7594fdad0/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e8073-366c-4d02-ace6-d6025a7eda0b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92622913-be86-42ff-aa98-239d3c3f45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Dataset definition\n",
    "# ---------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=30, pred_len=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data (pd.DataFrame or np.array): Time series with a column 'value'\n",
    "        - seq_len (int): Number of timesteps in the input sequence\n",
    "        - pred_len (int): Number of timesteps in the output sequence\n",
    "        \"\"\"\n",
    "        # Expecting a df with at least one column named \"value\"\n",
    "        self.data = np.array(data[\"value\"]) if isinstance(data, pd.DataFrame) else np.array(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # e.g. if data has 100 points, and seq_len=30, pred_len=1 -> total sequences ~ 69\n",
    "        return max(0, len(self.data) - self.seq_len - self.pred_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset length {len(self)}\")\n",
    "\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len : idx + self.seq_len + self.pred_len]\n",
    "\n",
    "        # Return tensors for PyTorch\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Model definition\n",
    "# ---------------------------\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        \"\"\"\n",
    "        A simple LSTM for univariate time series prediction.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, seq_len)\n",
    "        We'll unsqueeze(-1) to get (batch_size, seq_len, input_size=1).\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        # out is (batch_size, seq_len, hidden_size)\n",
    "        # We want the last timestep\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # shape (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Main training function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Channels for data paths (SageMaker will populate these automatically)\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--seq-len\", type=int, default=20)\n",
    "    parser.add_argument(\"--pred-len\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--hidden-size\", type=int, default=50)\n",
    "    parser.add_argument(\"--num-layers\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3a) Load data\n",
    "    # ---------------------------\n",
    "    # We assume your processor wrote \"train.csv\", \"validation.csv\", and \"test.csv\"\n",
    "    # into the respective directories: /opt/ml/input/data/train, etc.\n",
    "    train_csv = os.path.join(args.train, \"train.csv\")\n",
    "    val_csv   = os.path.join(args.validation, \"validation.csv\")\n",
    "    test_csv  = os.path.join(args.test, \"test.csv\")\n",
    "\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    val_data   = pd.read_csv(val_csv)\n",
    "    test_data  = pd.read_csv(test_csv)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3b) Create PyTorch datasets & loaders\n",
    "    # ---------------------------\n",
    "    train_dataset = TimeSeriesDataset(train_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    val_dataset   = TimeSeriesDataset(val_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    test_dataset  = TimeSeriesDataset(test_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3c) Initialize model, loss, optimizer\n",
    "    # ---------------------------\n",
    "    model = LSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3d) Training Loop\n",
    "    # ---------------------------\n",
    "    for epoch in range(args.epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{args.epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} \"\n",
    "              f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3e) (Optional) Test evaluation\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_targets in test_loader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            test_loss += criterion(test_outputs, test_targets).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3f) Save the model\n",
    "    # ---------------------------\n",
    "    # SageMaker automatically uploads /opt/ml/model to S3 after training\n",
    "    model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "877b6006-ffe5-4d60-9c0f-4ff5f4cb1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",  # folder containing train.py\n",
    "    role=role,\n",
    "    framework_version=\"2.0\",  # or whatever version suits your environment\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",  # or your choice\n",
    "    hyperparameters={\n",
    "        \"seq-len\": 20,\n",
    "        \"pred-len\": 1,\n",
    "        \"batch-size\": 8,\n",
    "        \"epochs\": 50,\n",
    "        \"hidden-size\": 50,\n",
    "        \"num-layers\": 1,\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9757f1cb-2436-4b1a-b03c-dc41debe563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TrainStep\",\n",
    "    estimator=pytorch_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27914dc9-12e8-4ac1-bb6c-6d418b6be5a6",
   "metadata": {},
   "source": [
    "## TESTING - OG VERSION ALL IN ONE PLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c4e63-137b-4e47-b1ea-807fb97502a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile code/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna\n",
    "\n",
    "SEQ_LEN = 20\n",
    "PRED_LEN = 1\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=30, pred_len=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data (pd.DataFrame or np.array): Time series values\n",
    "        - seq_len (int): Number of time steps in input sequence\n",
    "        - pred_len (int): Number of time steps in output sequence\n",
    "        \"\"\"\n",
    "        self.data = np.array(data[\"value\"]) if isinstance(data, pd.DataFrame) else np.array(data) \n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total number of sequences available\"\"\"\n",
    "        return max(0, len(self.data) - self.seq_len - self.pred_len) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves input sequence and target sequence\"\"\"\n",
    "        if idx >= len(self):  \n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset length {len(self)}\")\n",
    "\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len : idx + self.seq_len + self.pred_len]\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "val_dataset = TimeSeriesDataset(val_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "test_dataset = TimeSeriesDataset(test_data, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Using Optuna to do hyperparameter optimization\n",
    "    \"\"\"\n",
    "    # Hyperparameter search spaces\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)  \n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "    # Create model, loss, optimizer\n",
    "    model = LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Max number of epochs for each trial (unless doing early stopping)\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Compute average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        #trial.report(train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        #trial.report(val_loss, epoch)\n",
    "\n",
    "        # Early pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Value: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "best_model = LSTM(\n",
    "    input_size=1,\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    num_layers=best_params[\"num_layers\"]\n",
    ")\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        best_optimizer.zero_grad()\n",
    "        \n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_outputs = best_model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd3892-0348-4c3b-99bf-d3fa07f7b28e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c924a-ba5f-48ce-b301-c8d4e22ab697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0b22481-17c6-462d-979f-fc4cfa8a5f00",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "721c3b10-97dc-498f-a673-434e7e09e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"AirDataPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "    ],#, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "664e58bb-0ef1-46b9-9315-86ddb171a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-790237383528/airdata/sensor_data.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 0.5}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'AirDataProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-790237383528/AirDataPipeline/code/f6bb8f50726d68a202f70eb7594fdad0/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'TrainStep',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0-cpu-py310',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'validation'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'seq-len': '20',\n",
       "     'pred-len': '1',\n",
       "     'batch-size': '8',\n",
       "     'epochs': '50',\n",
       "     'hidden-size': '50',\n",
       "     'num-layers': '1',\n",
       "     'lr': '0.001',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-790237383528/TrainStep-ac841deddb2f8824b1729cb8958c85e5/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-east-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/',\n",
       "     'DisableProfiler': False}}}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb62c-20ad-41bf-9d18-46e48cfcd629",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3f8f37f-bf9b-4943-a5b1-62fe3ee2ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "try:\n",
    "    execution.wait()\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55336dbe-eb0a-4ee8-bcb3-476ae4e37cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:790237383528:pipeline/AirDataPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:790237383528:pipeline/AirDataPipeline/execution/q3qlrqub0ovl',\n",
       " 'PipelineExecutionDisplayName': 'execution-1739737701407',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'airdatapipeline',\n",
       "  'TrialName': 'q3qlrqub0ovl'},\n",
       " 'CreationTime': datetime.datetime(2025, 2, 16, 20, 28, 21, 333000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 2, 16, 20, 33, 42, 79000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:790237383528:user-profile/d-y5kcordfnuyc/kellerflint',\n",
       "  'UserProfileName': 'kellerflint',\n",
       "  'DomainId': 'd-y5kcordfnuyc',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::790237383528:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA3P7ORRNUBZEOQGRLH:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:790237383528:user-profile/d-y5kcordfnuyc/kellerflint',\n",
       "  'UserProfileName': 'kellerflint',\n",
       "  'DomainId': 'd-y5kcordfnuyc',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::790237383528:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA3P7ORRNUBZEOQGRLH:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': 'bfb4f080-99a8-4556-8f17-e33025b09bdb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bfb4f080-99a8-4556-8f17-e33025b09bdb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1062',\n",
       "   'date': 'Sun, 16 Feb 2025 20:41:53 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36eca0e3-1d62-4569-9f13-1d40e8b291d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'TrainStep',\n",
       "  'StartTime': datetime.datetime(2025, 2, 16, 20, 30, 56, 415000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 16, 20, 33, 41, 581000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:training-job/pipelines-q3qlrqub0ovl-TrainStep-N2jAzYQcvZ'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataProcess',\n",
       "  'StartTime': datetime.datetime(2025, 2, 16, 20, 28, 22, 398000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 16, 20, 30, 55, 825000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:processing-job/pipelines-q3qlrqub0ovl-AirDataProcess-GJphQo3mls'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9f127-24ee-4432-8400-d9dbe9026eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

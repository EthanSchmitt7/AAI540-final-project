{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107e4a0e-f06e-4860-b7d2-5d4a7e6b362f",
   "metadata": {},
   "source": [
    "# Model Pipeline\n",
    "\n",
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c1f117-97d3-4bd5-b8f8-32cc16ad0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (4.2.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "82152d6f-aa33-4412-82f9-ce24e50d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33e8befd-c4b9-4ec9-8c48-27c32ded23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74837e7a-6936-4f03-a47e-387fca8935d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-790237383528/airdata/sensor_data.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/sensor_data.csv\"\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/airdata\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec58ebf2-0092-4a54-a5e5-6a2dd287e0ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6219f-e675-4925-b3b4-54cf1650fbf6",
   "metadata": {},
   "source": [
    "## Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ce99344-a8fd-4aba-b6f7-103c475a8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TARGET_PARAM = \"pm25\"\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "df_location = pd.read_csv(f\"{base_dir}/input/sensor_data.csv\")\n",
    "\n",
    "# Split training\n",
    "df_param = df_location[df_location[\"parameter\"] == TARGET_PARAM]  # Filter data for this parameter\n",
    "train_data = df_param.iloc[: int(len(df_param) * TRAIN_SPLIT)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split validation\n",
    "val_data = df_param.iloc[\n",
    "    int(len(df_param) * TRAIN_SPLIT) : int(len(df_param) * TRAIN_SPLIT)\n",
    "    + int(len(df_param) * VAL_SPLIT)\n",
    "]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "# Split testing\n",
    "test_data = df_param.iloc[\n",
    "    int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT) : int(\n",
    "        len(df_param) * TRAIN_SPLIT\n",
    "    )\n",
    "    + int(len(df_param) * VAL_SPLIT)\n",
    "    + int(len(df_param) * TEST_SPLIT)\n",
    "]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "val_data.loc[:, \"value\"] = scaler.transform(val_data[\"value\"].values.reshape(-1, 1))\n",
    "test_data.loc[:, \"value\"] = scaler.transform(test_data[\"value\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "\n",
    "pd.DataFrame(train_data).to_csv(f\"{base_dir}/train/train.csv\")\n",
    "pd.DataFrame(val_data).to_csv(f\"{base_dir}/validation/validation.csv\")\n",
    "pd.DataFrame(test_data).to_csv(f\"{base_dir}/test/test.csv\")\n",
    "\n",
    "scaler_dir = f\"{base_dir}/scaler\"\n",
    "scaler_output_path = f\"{scaler_dir}/scaler.pkl\"\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "joblib.dump(scaler, scaler_output_path)\n",
    "print(f\"Scaler saved to {scaler_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef8d4734-6ac1-4c92-9341-c7f3751f11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-airdata-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ff14b4d6-2f66-49c2-bacf-67c2f49fff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"scaler\", source=\"/opt/ml/processing/scaler\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"AirDataProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e8073-366c-4d02-ace6-d6025a7eda0b",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "\n",
    "After processing features are uploaded to a Feature Store so they can be accessed in later projects or when re-training the model.\n",
    "\n",
    "Note: Code throughout the feature store section is based on the examples at https://github.com/mechristenson/aai-540-labs/tree/main/lab-3-1-sagemaker-feature-store . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ec1b62-1b5c-4e95-b9d6-bc1db7809e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\",\n",
    "                                       region_name=region)\n",
    "\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd44f24-7c27-40ab-bffb-f3afddc4562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-790237383528\n"
     ]
    }
   ],
   "source": [
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = \"airdata-featurestore\"\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29b541-2dbf-46b5-9792-3f42941a2ffd",
   "metadata": {},
   "source": [
    "### Process Features\n",
    "\n",
    "Pre-process and perform feature engineering before uploading features to Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90491e9b-2e1e-41a8-a42b-c2518cc42ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (700, 11)\n",
      "Validation Data Shape: (150, 11)\n",
      "Test Data Shape: (150, 11)\n"
     ]
    }
   ],
   "source": [
    "# Run pre-processor once\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TARGET_PARAM = \"pm25\"\n",
    "\n",
    "base_dir = \"data\"\n",
    "\n",
    "df_location = pd.read_csv(\n",
    "    f\"{base_dir}/sensor_data.csv\"\n",
    ")\n",
    "\n",
    "# Split training\n",
    "df_param = df_location[df_location['parameter'] == TARGET_PARAM]  # Filter data for this parameter\n",
    "train_data = df_param.iloc[:int(len(df_param) * TRAIN_SPLIT)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split validation\n",
    "val_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT)]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "# Split testing\n",
    "test_data = df_param.iloc[int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT):int(len(df_param) * TRAIN_SPLIT) + int(len(df_param) * VAL_SPLIT) + int(len(df_param) * TEST_SPLIT)]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "val_data.loc[:, \"value\"] = scaler.transform(val_data[\"value\"].values.reshape(-1, 1))\n",
    "test_data.loc[:, \"value\"] = scaler.transform(test_data[\"value\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b78be6-38be-4341-a3a9-946556861aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change id to string for better lookup\n",
    "\n",
    "train_data[\"measurement_id\"] = train_data[\"measurement_id\"].astype(str)  # Convert before ingestion\n",
    "val_data[\"measurement_id\"] = val_data[\"measurement_id\"].astype(str)  # Convert before ingestion\n",
    "test_data[\"measurement_id\"] = test_data[\"measurement_id\"].astype(str)  # Convert before ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad2274-d5e8-4ed7-8bc4-e4b334da3434",
   "metadata": {},
   "source": [
    "### Define Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a0b8a7-4401-4c2f-813d-1184082d4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "train_feature_group_name = \"train-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "val_feature_group_name = \"val-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "test_feature_group_name = \"test-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41eed4e4-9b6a-4325-aebd-c0d62a4233e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "train_feature_group = FeatureGroup(\n",
    "    name=train_feature_group_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "val_feature_group = FeatureGroup(\n",
    "    name=val_feature_group_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "test_feature_group = FeatureGroup(\n",
    "    name=test_feature_group_name, sagemaker_session=feature_store_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eb60815-bcf7-4315-9380-652f9252483b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='measurement_id', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='sensor_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='location_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='location', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='latitude', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='longitude', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='epoch', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='duration', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='parameter', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='value', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='units', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='EventTime', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "\n",
    "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
    "cast_object_to_string(train_data)\n",
    "cast_object_to_string(val_data)\n",
    "cast_object_to_string(test_data)\n",
    "\n",
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"measurement_id\"\n",
    "event_time_feature_name = \"EventTime\"\n",
    "\n",
    "# append EventTime feature\n",
    "train_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(train_data), dtype=\"float64\"\n",
    ")\n",
    "val_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(val_data), dtype=\"float64\"\n",
    ")\n",
    "test_data[event_time_feature_name] = pd.Series(\n",
    "    [current_time_sec] * len(val_data), dtype=\"float64\"\n",
    ")\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "train_feature_group.load_feature_definitions(data_frame=train_data)\n",
    "# output is suppressed\n",
    "val_feature_group.load_feature_definitions(data_frame=val_data)\n",
    "# output is suppressed\n",
    "test_feature_group.load_feature_definitions(data_frame=test_data)\n",
    "# output is suppressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d7242-ed9b-48cd-8ad4-b2a3cf353e5d",
   "metadata": {},
   "source": [
    "### Create FeatureGroups in SageMaker FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2627faae-66d5-49e9-a64d-5310ffab83ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup train-feature-group-23-20-21-42 successfully created.\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup val-feature-group-23-20-21-42 successfully created.\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup test-feature-group-23-20-21-42 successfully created.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "train_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "val_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "test_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=train_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=val_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=test_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb19125b-1321-4eb0-8a71-0aeab0bb7c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:790237383528:feature-group/train-feature-group-23-20-21-42',\n",
       " 'FeatureGroupName': 'train-feature-group-23-20-21-42',\n",
       " 'RecordIdentifierFeatureName': 'measurement_id',\n",
       " 'EventTimeFeatureName': 'EventTime',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'measurement_id',\n",
       "   'FeatureType': 'String'},\n",
       "  {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'location_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'location', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'latitude', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'longitude', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'epoch', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'duration', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'parameter', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'value', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'units', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2025, 2, 23, 20, 21, 43, 124000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-790237383528/airdata-featurestore',\n",
       "   'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-790237383528/airdata-featurestore/790237383528/sagemaker/us-east-1/offline-store/train-feature-group-23-20-21-42-1740342103/data'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'train_feature_group_23_20_21_42_1740342103',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'ThroughputConfig': {'ThroughputMode': 'OnDemand'},\n",
       " 'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'OnlineStoreTotalSizeBytes': 0,\n",
       " 'ResponseMetadata': {'RequestId': '2046c7b4-f05e-4851-b4c7-7aaaaafad791',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2046c7b4-f05e-4851-b4c7-7aaaaafad791',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2042',\n",
       "   'date': 'Sun, 23 Feb 2025 20:22:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926b04c-540d-4b7f-9b35-52cc3004f16e",
   "metadata": {},
   "source": [
    "### Put Records Into Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde33bad-aefd-48c7-aa85-eceec2a24534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='train-feature-group-23-20-21-42', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f6579166250>, sagemaker_session=<sagemaker.session.Session object at 0x7f6575b27d90>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f656f68ba90>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_group.ingest(data_frame=train_data, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c534c7-38ae-439a-b953-22e8aa6ae790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='val-feature-group-23-20-21-42', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f6579166250>, sagemaker_session=<sagemaker.session.Session object at 0x7f6575b27d90>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f656f66c190>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_feature_group.ingest(data_frame=val_data, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170c72f5-7613-457c-b938-613fb6ff160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='test-feature-group-23-20-21-42', feature_definitions={'measurement_id': {'FeatureName': 'measurement_id', 'FeatureType': 'String'}, 'sensor_id': {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'}, 'location_id': {'FeatureName': 'location_id', 'FeatureType': 'Integral'}, 'location': {'FeatureName': 'location', 'FeatureType': 'String'}, 'latitude': {'FeatureName': 'latitude', 'FeatureType': 'Fractional'}, 'longitude': {'FeatureName': 'longitude', 'FeatureType': 'Fractional'}, 'epoch': {'FeatureName': 'epoch', 'FeatureType': 'String'}, 'duration': {'FeatureName': 'duration', 'FeatureType': 'String'}, 'parameter': {'FeatureName': 'parameter', 'FeatureType': 'String'}, 'value': {'FeatureName': 'value', 'FeatureType': 'Fractional'}, 'units': {'FeatureName': 'units', 'FeatureType': 'String'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f6579166250>, sagemaker_session=<sagemaker.session.Session object at 0x7f6575b27d90>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f656f6d4390>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature_group.ingest(data_frame=test_data, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4c8c5-7502-4926-8dad-6b9808a76b3c",
   "metadata": {},
   "source": [
    "### Test both feature group and individual record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23781270-9854-42e8-b816-b55849931277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2025, 2, 23, 20, 21, 43, 124000, tzinfo=tzlocal()),\n",
      " 'EventTimeFeatureName': 'EventTime',\n",
      " 'FeatureDefinitions': [{'FeatureName': 'measurement_id',\n",
      "                         'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'sensor_id', 'FeatureType': 'Integral'},\n",
      "                        {'FeatureName': 'location_id',\n",
      "                         'FeatureType': 'Integral'},\n",
      "                        {'FeatureName': 'location', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'latitude',\n",
      "                         'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'longitude',\n",
      "                         'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'epoch', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'duration', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'parameter', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'value', 'FeatureType': 'Fractional'},\n",
      "                        {'FeatureName': 'units', 'FeatureType': 'String'},\n",
      "                        {'FeatureName': 'EventTime',\n",
      "                         'FeatureType': 'Fractional'}],\n",
      " 'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:790237383528:feature-group/train-feature-group-23-20-21-42',\n",
      " 'FeatureGroupName': 'train-feature-group-23-20-21-42',\n",
      " 'FeatureGroupStatus': 'Created',\n",
      " 'OfflineStoreConfig': {'DataCatalogConfig': {'Catalog': 'AwsDataCatalog',\n",
      "                                              'Database': 'sagemaker_featurestore',\n",
      "                                              'TableName': 'train_feature_group_23_20_21_42_1740342103'},\n",
      "                        'DisableGlueTableCreation': False,\n",
      "                        'S3StorageConfig': {'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-790237383528/airdata-featurestore/790237383528/sagemaker/us-east-1/offline-store/train-feature-group-23-20-21-42-1740342103/data',\n",
      "                                            'S3Uri': 's3://sagemaker-us-east-1-790237383528/airdata-featurestore'}},\n",
      " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
      " 'OnlineStoreTotalSizeBytes': 0,\n",
      " 'RecordIdentifierFeatureName': 'measurement_id',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '2042',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 23 Feb 2025 20:22:26 GMT',\n",
      "                                      'x-amzn-requestid': 'de127b13-d087-42fc-82b8-c770e6db35ce'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'de127b13-d087-42fc-82b8-c770e6db35ce',\n",
      "                      'RetryAttempts': 0},\n",
      " 'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
      " 'ThroughputConfig': {'ThroughputMode': 'OnDemand'}}\n"
     ]
    }
   ],
   "source": [
    "# Test that feature groups were made correctly\n",
    "\n",
    "import pprint\n",
    "\n",
    "response = sagemaker_client.describe_feature_group(FeatureGroupName=train_feature_group_name)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a00c991-f012-4d07-9574-e640aa0f5642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Record': [{'FeatureName': 'measurement_id', 'ValueAsString': '2302'},\n",
      "            {'FeatureName': 'sensor_id', 'ValueAsString': '2000855'},\n",
      "            {'FeatureName': 'location_id', 'ValueAsString': '947312'},\n",
      "            {'FeatureName': 'location', 'ValueAsString': 'Canyon ES (2795)'},\n",
      "            {'FeatureName': 'latitude', 'ValueAsString': '34.03213'},\n",
      "            {'FeatureName': 'longitude', 'ValueAsString': '-118.51198'},\n",
      "            {'FeatureName': 'epoch', 'ValueAsString': '2022-02-19 06:36:52'},\n",
      "            {'FeatureName': 'duration', 'ValueAsString': '0 days 00:03:00'},\n",
      "            {'FeatureName': 'parameter', 'ValueAsString': 'pm25'},\n",
      "            {'FeatureName': 'value', 'ValueAsString': '-0.46964278273851745'},\n",
      "            {'FeatureName': 'units', 'ValueAsString': 'µg/m³'},\n",
      "            {'FeatureName': 'EventTime', 'ValueAsString': '1740342103.0'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1013',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Sun, 23 Feb 2025 20:22:26 GMT',\n",
      "                                      'x-amzn-requestid': '09ab4508-f7f2-4815-8b60-fb1ac8195158'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '09ab4508-f7f2-4815-8b60-fb1ac8195158',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Test that individual records can be accessed\n",
    "\n",
    "record_identifier_value = str(2302)\n",
    "\n",
    "record = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=train_feature_group_name,\n",
    "    RecordIdentifierValueAsString=record_identifier_value,\n",
    ")\n",
    "\n",
    "pprint.pprint(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d3a58-5235-4c45-931c-5f86f5e72af3",
   "metadata": {},
   "source": [
    "A record can be successfully pulled from the Feature Store, indicating that features are available for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582295e-aa38-4916-a324-457a52ab65c7",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "92622913-be86-42ff-aa98-239d3c3f45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/train.py\n",
    "\n",
    "import shutil\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset definition\n",
    "# ---------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=30, pred_len=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data (pd.DataFrame or np.array): Time series with a column 'value'\n",
    "        - seq_len (int): Number of timesteps in the input sequence\n",
    "        - pred_len (int): Number of timesteps in the output sequence\n",
    "        \"\"\"\n",
    "        self.data = np.array(data[\"value\"]) if isinstance(data, pd.DataFrame) else np.array(data)\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total number of sequences available\"\"\"\n",
    "        return max(0, len(self.data) - self.seq_len - self.pred_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves input sequence and target sequence\"\"\"\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset length {len(self)}\")\n",
    "\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len : idx + self.seq_len + self.pred_len]\n",
    "\n",
    "        # Return tensors for PyTorch\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model definition\n",
    "# ---------------------------\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        # out is (batch_size, seq_len, hidden_size)\n",
    "        # We want the last timestep\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # shape (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main training function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Channels for data paths (SageMaker will populate these automatically)\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--scaler\", type=str, default=os.environ.get(\"SM_CHANNEL_SCALER\"))\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--seq-len\", type=int, default=20)\n",
    "    parser.add_argument(\"--pred-len\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=8)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--hidden-size\", type=int, default=50)\n",
    "    parser.add_argument(\"--num-layers\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Load data\n",
    "    # ---------------------------\n",
    "    # Processor wrote \"train.csv\", \"validation.csv\", and \"test.csv\"\n",
    "    # into the respective directories: /opt/ml/input/data/train, etc.\n",
    "    train_csv = os.path.join(args.train, \"train.csv\")\n",
    "    val_csv = os.path.join(args.validation, \"validation.csv\")\n",
    "    test_csv = os.path.join(args.test, \"test.csv\")\n",
    "    scaler_source_path = os.path.join(args.scaler, \"scaler.pkl\")\n",
    "\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    val_data = pd.read_csv(val_csv)\n",
    "    test_data = pd.read_csv(test_csv)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Create PyTorch datasets & loaders\n",
    "    # ---------------------------\n",
    "    train_dataset = TimeSeriesDataset(train_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    val_dataset = TimeSeriesDataset(val_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "    test_dataset = TimeSeriesDataset(test_data, seq_len=args.seq_len, pred_len=args.pred_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Initialize model, loss, optimizer\n",
    "    # ---------------------------\n",
    "    model = LSTM(input_size=1, hidden_size=args.hidden_size, num_layers=args.num_layers)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training Loop\n",
    "    # ---------------------------\n",
    "    for epoch in range(args.epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_outputs = model(test_inputs)\n",
    "                test_loss += criterion(test_outputs, test_targets).item()\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{args.epochs}] \"\n",
    "            f\"TrainLoss: {train_loss:.4f} \"\n",
    "            f\"ValLoss: {val_loss:.4f} \"\n",
    "            f\"TestLoss: {test_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save the model\n",
    "    # ---------------------------\n",
    "    # Uploads /opt/ml/model to S3 after training\n",
    "    model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    # Copy scaler values to model directory\n",
    "    if not os.path.exists(scaler_source_path):\n",
    "        raise FileNotFoundError(f\"Scaler file not found at {scaler_source_path}\")\n",
    "    scaler_destination_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "    shutil.copy(scaler_source_path, scaler_destination_path)\n",
    "    print(f\"Scaler copied to {scaler_destination_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "877b6006-ffe5-4d60-9c0f-4ff5f4cb1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters={\n",
    "        \"seq-len\": 20,\n",
    "        \"pred-len\": 1,\n",
    "        \"batch-size\": 8,\n",
    "        \"epochs\": 50,\n",
    "        \"hidden-size\": 50,\n",
    "        \"num-layers\": 1,\n",
    "        \"lr\": 0.001,\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    "    metric_definitions=[{\"Name\": \"test_mse\", \"Regex\": r\"TestLoss:\\s+([0-9\\.]+)\"}],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9757f1cb-2436-4b1a-b03c-dc41debe563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TrainStep\",\n",
    "    estimator=pytorch_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri\n",
    "        ),\n",
    "        \"scaler\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"scaler\"].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714b92b-3417-4ef8-a250-496c63483ab2",
   "metadata": {},
   "source": [
    "## Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63ba76a1-617a-4696-b040-03314328a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# Initialize CloudWatch client\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name  # Dynamically get region from the AWS environment\n",
    "cloudwatch = boto3.client(\"cloudwatch\", region_name=region_name)  # Change region as needed\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x.unsqueeze(-1), (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(\"Running model_fn\")\n",
    "    model = LSTM(input_size=1, hidden_size=80, num_layers=2)\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "\n",
    "    # Load scaler\n",
    "    scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "    global scaler\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    print(\"Running input_fn\")\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)[\"data\"]\n",
    "\n",
    "        raw_inputs = np.array(data).reshape(-1, 1)\n",
    "        scaled_inputs = scaler.transform(raw_inputs)\n",
    "        inputs = torch.tensor(scaled_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Log raw input data to CloudWatch\n",
    "        cloudwatch.put_metric_data(\n",
    "            Namespace=\"AirQualityMonitoring\",\n",
    "            MetricData=[{\"MetricName\": \"RawQueriedPM25Value\", \"Unit\": \"None\", \"Value\": data[0]}],\n",
    "        )\n",
    "        return inputs\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    print(\"Running predict_fn\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        preds = model(input_object)\n",
    "    inference_time = time.time() - start_time\n",
    "    input_value = input_object.squeeze().tolist()\n",
    "\n",
    "    # Log prediction latency and input to model\n",
    "    cloudwatch.put_metric_data(\n",
    "        Namespace=\"AirQualityMonitoring\",\n",
    "        MetricData=[\n",
    "            {\n",
    "                \"MetricName\": \"InputPM25Value\",\n",
    "                \"Unit\": \"None\",\n",
    "                \"Value\": input_value[0] if isinstance(input_value, list) else input_value,\n",
    "            },\n",
    "            {\n",
    "                \"MetricName\": \"InferenceLatency\",\n",
    "                \"Unit\": \"Milliseconds\",\n",
    "                \"Value\": inference_time * 1000,  # Convert to ms\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return preds\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    print(\"Running output_fn\")\n",
    "    if response_content_type == \"application/json\":\n",
    "        scaled_result = prediction.squeeze().item()\n",
    "        # Convert prediction to original scale\n",
    "        result = scaler.inverse_transform(np.array(scaled_result).reshape(-1, 1)).item()\n",
    "\n",
    "        # Log prediction value\n",
    "        cloudwatch.put_metric_data(\n",
    "            Namespace=\"AirQualityMonitoring\",\n",
    "            MetricData=[{\"MetricName\": \"PredictedPM25\", \"Unit\": \"None\", \"Value\": result}],\n",
    "        )\n",
    "        return json.dumps({\"prediction\": result})\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported response content type: {response_content_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2afe3020-97f4-4e37-97e5-71d6271260ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "inference_model = PyTorchModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    ")\n",
    "\n",
    "step_create = ModelStep(\n",
    "    name=\"AirDataCreateModel\",\n",
    "    step_args=inference_model.create(instance_type=\"ml.m5.large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b3f5ae7-c26c-4be8-bf51-59c09a45445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = None  # metrics we want?\n",
    "register_args = inference_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=\"AirDataModelGroup\",\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"RegisterAirDataModel\", step_args=register_args)\n",
    "\n",
    "step_register = ModelStep(name=\"AirDataRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd3892-0348-4c3b-99bf-d3fa07f7b28e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "85486c31-2539-4c49-bf40-cf136968528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"MSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
    ")\n",
    "\n",
    "test_mse = step_train.properties.FinalMetricDataList[0].Value\n",
    "\n",
    "step_check_loss = ConditionStep(\n",
    "    name=\"CheckTestLoss\",\n",
    "    conditions=[ConditionLessThanOrEqualTo(left=test_mse, right=mse_threshold)],\n",
    "    if_steps=[step_create, step_register],\n",
    "    else_steps=[step_fail],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b22481-17c6-462d-979f-fc4cfa8a5f00",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "721c3b10-97dc-498f-a673-434e7e09e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"AirDataPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "        step_check_loss,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "664e58bb-0ef1-46b9-9315-86ddb171a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-790237383528/airdata/sensor_data.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 0.5}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'AirDataProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-790237383528/AirDataPipeline/code/9dc52c9568f8166e3f2819fe07ff7775/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'scaler',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-790237383528',\n",
       "           'AirDataPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'AirDataProcess',\n",
       "           'output',\n",
       "           'scaler']}},\n",
       "        'LocalPath': '/opt/ml/processing/scaler',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'TrainStep',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0-cpu-py310',\n",
       "     'MetricDefinitions': [{'Name': 'test_mse',\n",
       "       'Regex': 'TestLoss:\\\\s+([0-9\\\\.]+)'}],\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'validation'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.AirDataProcess.ProcessingOutputConfig.Outputs['scaler'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'scaler'}],\n",
       "    'HyperParameters': {'seq-len': '20',\n",
       "     'pred-len': '1',\n",
       "     'batch-size': '8',\n",
       "     'epochs': '50',\n",
       "     'hidden-size': '50',\n",
       "     'num-layers': '1',\n",
       "     'lr': '0.001',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-790237383528/TrainStep-b54a326934da9638efb3d4cbe2fa6e31/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-east-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/',\n",
       "     'DisableProfiler': False}}},\n",
       "  {'Name': 'CheckTestLoss',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Get': 'Steps.TrainStep.FinalMetricDataList[0].Value'},\n",
       "      'RightValue': {'Get': 'Parameters.MseThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'AirDataCreateModel-RepackModel-0',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/pytorch-inference-2025-02-23-21-51-26-673'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "        'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large'},\n",
       "       'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': {'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'},\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"inference.py\"',\n",
       "        'model_archive': {'Std:Join': {'On': '',\n",
       "          'Values': [{'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': '\"code\"',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-790237383528/AirDataCreateModel-RepackModel-0-b54a326934da9638efb3d4cbe2fa6e31/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_region': '\"us-east-1\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/pytorch-inference-2025-02-23-21-51-26-673',\n",
       "        'CollectionConfigurations': []},\n",
       "       'ProfilerConfig': {'DisableProfiler': True}},\n",
       "      'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "     {'Name': 'AirDataCreateModel-CreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "       'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0-cpu-py310',\n",
       "        'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "         'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "         'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "         'SAGEMAKER_REGION': 'us-east-1'},\n",
       "        'ModelDataUrl': {'Get': 'Steps.AirDataCreateModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "     {'Name': 'AirDataRegisterModel-RepackModel-0',\n",
       "      'Type': 'Training',\n",
       "      'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "        'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "       'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/pytorch-inference-2025-02-23-21-51-27-713'},\n",
       "       'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "       'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "        'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.large'},\n",
       "       'RoleArn': 'arn:aws:iam::790237383528:role/LabRole',\n",
       "       'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "           'S3Uri': {'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'},\n",
       "           'S3DataDistributionType': 'FullyReplicated'}},\n",
       "         'ChannelName': 'training'}],\n",
       "       'HyperParameters': {'inference_script': '\"inference.py\"',\n",
       "        'model_archive': {'Std:Join': {'On': '',\n",
       "          'Values': [{'Get': 'Steps.TrainStep.ModelArtifacts.S3ModelArtifacts'}]}},\n",
       "        'dependencies': 'null',\n",
       "        'source_dir': '\"code\"',\n",
       "        'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-790237383528/AirDataRegisterModel-RepackModel-0-b54a326934da9638efb3d4cbe2fa6e31/source/sourcedir.tar.gz\"',\n",
       "        'sagemaker_program': '\"_repack_script_launcher.sh\"',\n",
       "        'sagemaker_container_log_level': '20',\n",
       "        'sagemaker_region': '\"us-east-1\"'},\n",
       "       'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-790237383528/pytorch-inference-2025-02-23-21-51-27-713',\n",
       "        'CollectionConfigurations': []},\n",
       "       'ProfilerConfig': {'DisableProfiler': True}},\n",
       "      'Description': 'Used to repack a model with customer scripts for a register/create model step'},\n",
       "     {'Name': 'AirDataRegisterModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'AirDataModelGroup',\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0-cpu-py310',\n",
       "          'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "           'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code',\n",
       "           'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "           'SAGEMAKER_REGION': 'us-east-1'},\n",
       "          'ModelDataUrl': {'Get': 'Steps.AirDataRegisterModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts'},\n",
       "          'Framework': 'PYTORCH',\n",
       "          'FrameworkVersion': '2.0'}],\n",
       "        'SupportedContentTypes': ['application/json'],\n",
       "        'SupportedResponseMIMETypes': ['application/json'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
       "       'SkipModelValidation': 'None'}}],\n",
       "    'ElseSteps': [{'Name': 'MSEFail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to MSE >',\n",
       "          {'Get': 'Parameters.MseThreshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fb62c-20ad-41bf-9d18-46e48cfcd629",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f3f8f37f-bf9b-4943-a5b1-62fe3ee2ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "try:\n",
    "    execution.wait()\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55336dbe-eb0a-4ee8-bcb3-476ae4e37cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:790237383528:pipeline/AirDataPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:790237383528:pipeline/AirDataPipeline/execution/0snvhm0omx86',\n",
       " 'PipelineExecutionDisplayName': 'execution-1740347491400',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'airdatapipeline',\n",
       "  'TrialName': '0snvhm0omx86'},\n",
       " 'CreationTime': datetime.datetime(2025, 2, 23, 21, 51, 31, 325000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 2, 23, 21, 59, 14, 611000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:790237383528:user-profile/d-y5kcordfnuyc/kellerflint',\n",
       "  'UserProfileName': 'kellerflint',\n",
       "  'DomainId': 'd-y5kcordfnuyc',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::790237383528:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA3P7ORRNUBZEOQGRLH:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:790237383528:user-profile/d-y5kcordfnuyc/kellerflint',\n",
       "  'UserProfileName': 'kellerflint',\n",
       "  'DomainId': 'd-y5kcordfnuyc',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::790237383528:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA3P7ORRNUBZEOQGRLH:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': '6adca454-784c-46d2-90e1-41d32f3346d5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6adca454-784c-46d2-90e1-41d32f3346d5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1062',\n",
       "   'date': 'Sun, 23 Feb 2025 22:01:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36eca0e3-1d62-4569-9f13-1d40e8b291d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'AirDataCreateModel-CreateModel',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 59, 12, 721000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 59, 14, 369000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:model/pipelines-0snvhm0omx86-AirDataCreateModel-C-2BM8neBf4k'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataRegisterModel-RegisterModel',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 59, 10, 176000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 59, 11, 129000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:model-package/AirDataModelGroup/5'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataCreateModel-RepackModel-0',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 57, 3, 760000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 59, 12, 30000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:training-job/pipelines-0snvhm0omx86-AirDataCreateModel-R-dUtdaxNIq8'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataRegisterModel-RepackModel-0',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 57, 3, 760000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 59, 9, 630000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:training-job/pipelines-0snvhm0omx86-AirDataRegisterModel-XdhmTdkwaX'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'CheckTestLoss',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 57, 3, 33000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 57, 3, 360000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Condition': {'Outcome': 'True'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'TrainStep',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 54, 11, 662000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 57, 2, 377000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:training-job/pipelines-0snvhm0omx86-TrainStep-XqFeBjLJpF'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'AirDataProcess',\n",
       "  'StartTime': datetime.datetime(2025, 2, 23, 21, 51, 32, 53000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2025, 2, 23, 21, 54, 10, 976000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:790237383528:processing-job/pipelines-0snvhm0omx86-AirDataProcess-lhCKnHxICR'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af7c11-d4bd-4bb5-a02a-d438fe5ce614",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5c84abe3-0cc9-41bb-bed8-9855c78b9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent model package ARN: arn:aws:sagemaker:us-east-1:790237383528:model-package/AirDataModelGroup/5\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "MODEL_PACKAGE_GROUP = \"AirDataModelGroup\"\n",
    "\n",
    "# Get all model packages in the group, sorted by creation time\n",
    "model_packages = sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n",
    "    SortBy=\"CreationTime\",\n",
    "    SortOrder=\"Descending\",\n",
    ")[\"ModelPackageSummaryList\"]\n",
    "\n",
    "if not model_packages:\n",
    "    raise ValueError(f\"No models found in Model Package Group: {MODEL_PACKAGE_GROUP}\")\n",
    "\n",
    "# Get the most recent model package ARN\n",
    "model_package_arn = model_packages[0][\"ModelPackageArn\"]\n",
    "\n",
    "print(f\"Most recent model package ARN: {model_package_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0ce1bd41-cd82-4e66-b5f0-44edcf737daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: AirDataModelGroup-2025-02-23-22-19-34-579\n",
      "INFO:sagemaker:Creating endpoint-config with name AirDataModelGroup-2025-02-23-22-19-36-012\n",
      "INFO:sagemaker:Creating endpoint with name AirDataModelGroup-2025-02-23-22-19-36-012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "deployment_model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    ")\n",
    "\n",
    "predictor = deployment_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3163d-6b60-48c8-bb8e-398565a00cdb",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "38fcb482-0e95-4046-b9e8-494de9310b1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeoutError",
     "evalue": "Read timeout on endpoint URL: \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/AirDataModelGroup-2025-02-23-22-19-36-012/invocations\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:801\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 801\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/retry.py:527\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m error:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:358\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: AWSHTTPSConnectionPool(host='runtime.sagemaker.us-east-1.amazonaws.com', port=443): Read timed out. (read timeout=60)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m prod_X \u001b[38;5;241m=\u001b[39m production_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m input_data \u001b[38;5;241m=\u001b[39m prod_X[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAirDataModelGroup-2025-02-23-22-19-36-012\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[138], line 4\u001b[0m, in \u001b[0;36mquery_endpoint\u001b[0;34m(endpoint_name, input_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery_endpoint\u001b[39m(endpoint_name, input_data):\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mruntime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1005\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     maybe_compress_request(\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m   1003\u001b[0m     )\n\u001b[1;32m   1004\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m-> 1005\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1015\u001b[0m )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1029\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1033\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1034\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1035\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:200\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[1;32m    197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    198\u001b[0m     request, operation_model, context\n\u001b[1;32m    199\u001b[0m )\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_needs_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuccess_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    207\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts, success_response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:360\u001b[0m, in \u001b[0;36mEndpoint._needs_retry\u001b[0;34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[0m\n\u001b[1;32m    358\u001b[0m service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[1;32m    359\u001b[0m event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneeds-retry.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 360\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaught_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaught_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m handler_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maliased_event_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:207\u001b[0m, in \u001b[0;36mRetryHandler.__call__\u001b[0;34m(self, attempts, response, caught_exception, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     retries_context \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest_dict\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretries\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    205\u001b[0m     checker_kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretries_context\u001b[39m\u001b[38;5;124m'\u001b[39m: retries_context})\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchecker_kwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action(attempts\u001b[38;5;241m=\u001b[39mattempts)\n\u001b[1;32m    209\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry needed, action of: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:284\u001b[0m, in \u001b[0;36mMaxAttemptsDecorator.__call__\u001b[0;34m(self, attempt_number, response, caught_exception, retries_context)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries_context:\n\u001b[1;32m    280\u001b[0m     retries_context[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    281\u001b[0m         retries_context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_attempts\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m should_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_retry:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attempt_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_attempts:\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;66;03m# explicitly set MaxAttemptsReached\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:320\u001b[0m, in \u001b[0;36mMaxAttemptsDecorator._should_retry\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# If we've exceeded the max attempts we just let the exception\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# propagate if one has occurred.\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:363\u001b[0m, in \u001b[0;36mMultiChecker.__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attempt_number, response, caught_exception):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkers:\n\u001b[0;32m--> 363\u001b[0m         checker_response \u001b[38;5;241m=\u001b[39m \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m checker_response:\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m checker_response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:247\u001b[0m, in \u001b[0;36mBaseChecker.__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(attempt_number, response)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m caught_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_caught_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattempt_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaught_exception\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth response and caught_exception are None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/retryhandler.py:416\u001b[0m, in \u001b[0;36mExceptionRaiser._check_caught_exception\u001b[0;34m(self, attempt_number, caught_exception)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_caught_exception\u001b[39m(\u001b[38;5;28mself\u001b[39m, attempt_number, caught_exception):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# This is implementation specific, but this class is useful by\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# coordinating with the MaxAttemptsDecorator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# the MaxAttemptsDecorator is not interested in retrying the exception\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# then this exception just propagates out past the retry code.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m caught_exception\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:279\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:383\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/httpsession.py:501\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(endpoint_url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl, error\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(endpoint_url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl, error\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectionClosedError(\n\u001b[1;32m    504\u001b[0m         error\u001b[38;5;241m=\u001b[39me, request\u001b[38;5;241m=\u001b[39mrequest, endpoint_url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m    505\u001b[0m     )\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: Read timeout on endpoint URL: \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/AirDataModelGroup-2025-02-23-22-19-36-012/invocations\""
     ]
    }
   ],
   "source": [
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "def query_endpoint(endpoint_name, input_data):\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps({\"data\": input_data}),\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode())\n",
    "    print(\"Prediction response:\", result)\n",
    "    return result\n",
    "\n",
    "production_data = pd.read_csv(\"data/production_data.csv\")\n",
    "prod_X = production_data[\"value\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "input_data = prod_X[0].tolist()\n",
    "result = query_endpoint(\"AirDataModelGroup-2025-02-23-22-19-36-012\", input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4735ec-d4b7-4889-a5ac-27e5062c4943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

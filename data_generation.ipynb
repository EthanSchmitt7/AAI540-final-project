{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "This script will generate the necessary files to construct the pipeline and simulate production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from openaq import OpenAQ\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.4\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "PROD_SPLIT = 0.4\n",
    "\n",
    "SEQ_LEN = 20\n",
    "PRED_LEN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PARAM = \"pm25\"\n",
    "LOCATIONS_DICT = {\n",
    "    \"Canyon ES (2795)\": 947312,\n",
    "    \"Pacific Palisades ES (5959)\": 947232,\n",
    "    \"Revere MS (8356)\": 947280,\n",
    "    \"Brentwood Sci Mag ES (2507)\": 947305,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your OpenAQ API key in a text file with the name \"openaq_api_key.txt\"\n",
    "with open(\"api_keys/openaq_api_key.txt\", \"r\") as file:\n",
    "    API_KEY = file.read()\n",
    "\n",
    "# Initialize the OpenAQ client\n",
    "client = OpenAQ(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sensors_by_list(sensor_list):\n",
    "    format_string = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    data = {}\n",
    "    m_id = 0\n",
    "    # For each location in the response, fetch its sensors\n",
    "    for id in sensor_list:\n",
    "        location = client.locations.get(id).results[0]\n",
    "        print(f\"\"\"Fetching {location.name} data\"\"\")\n",
    "        for sensor in location.sensors:\n",
    "            lat = location.coordinates.latitude\n",
    "            long = location.coordinates.longitude\n",
    "            loc_name = location.name\n",
    "            location_id = location.id\n",
    "            sensor = sensor.id\n",
    "\n",
    "            # Fetch the recent measurements the sensor\n",
    "            measurements = client.measurements.list(sensor)\n",
    "\n",
    "            # For each measurement, record the relevant data\n",
    "            for measurement in measurements.results:\n",
    "                m_id += 1\n",
    "                epoch = datetime.strptime(measurement.period.datetime_from.utc, format_string)\n",
    "                duration = timedelta(seconds=pd.to_timedelta(measurement.period.interval).seconds)\n",
    "                parameter = measurement.parameter.name\n",
    "                value = measurement.value\n",
    "                units = measurement.parameter.units\n",
    "\n",
    "                data[m_id] = {\n",
    "                    \"measurement_id\": m_id,\n",
    "                    \"sensor_id\": sensor,\n",
    "                    \"location_id\": location_id,\n",
    "                    \"location\": loc_name,\n",
    "                    \"latitude\": lat,\n",
    "                    \"longitude\": long,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"duration\": duration,\n",
    "                    \"parameter\": parameter,\n",
    "                    \"value\": value,\n",
    "                    \"units\": units,\n",
    "                }\n",
    "\n",
    "    return pd.DataFrame.from_dict(data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Canyon ES (2795) data\n"
     ]
    }
   ],
   "source": [
    "# Get only the first location for time series analysis\n",
    "location_index = LOCATIONS_DICT[\"Canyon ES (2795)\"]\n",
    "df = fetch_sensors_by_list([location_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for this location\n",
    "df_location = df[df[\"location_id\"] == location_index]\n",
    "df_param = df_location[df_location[\"parameter\"] == TARGET_PARAM]  # Filter data for this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training\n",
    "train_data = df_param.iloc[: int(len(df_param) * (TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT))]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split production\n",
    "prod_data = df_param.iloc[int(len(df_param) * (TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT)) :]\n",
    "prod_data = prod_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training dataset\n",
    "scaler = StandardScaler()\n",
    "train_data.loc[:, \"value\"] = scaler.fit_transform(train_data[\"value\"].values.reshape(-1, 1))\n",
    "prod_data.loc[:, \"value\"] = scaler.transform(prod_data[\"value\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to csv\n",
    "train_data.to_csv(\"data/sensor_data.csv\", index=False)\n",
    "prod_data.to_csv(\"data/production_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
